{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1266c333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "<frozen runpy>:128: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/nandhinirajasekaran/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m66.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!pip install -q sentence-transformers scikit-learn spacy nltk\n",
    "!python -m nltk.downloader punkt\n",
    "!python -m spacy download en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee9da389",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nandhinirajasekaran/Desktop/LLM/RAFT/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/nandhinirajasekaran/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import spacy\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed3901a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"raft_qa_dataset_semantic.json\") as f:\n",
    "    qa_dataset = json.load(f)\n",
    "\n",
    "chunks = list({(qa['source_file'], qa['plan_type']): [] for qa in qa_dataset})  # Placeholder for grouping by doc\n",
    "questions = [qa['question'] for qa in qa_dataset]\n",
    "answers = [qa['answer'] for qa in qa_dataset]\n",
    "all_qa_texts = [qa['question'] + \" \" + qa['answer'] for qa in qa_dataset]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a396541",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def spacy_sent_tokenize(text):\n",
    "    doc = nlp(text)\n",
    "    return [sent.text.strip() for sent in doc.sents]\n",
    "\n",
    "def chunk_coherence_score(chunks, model):\n",
    "    scores = []\n",
    "    for chunk in chunks:\n",
    "        sentences = spacy_sent_tokenize(chunk)\n",
    "        if len(sentences) < 2:\n",
    "            scores.append(1.0)\n",
    "            continue\n",
    "        embeddings = model.encode(sentences)\n",
    "        sims = [cosine_similarity([embeddings[i]], [embeddings[j]])[0][0]\n",
    "                for i in range(len(embeddings)) for j in range(i+1, len(embeddings))]\n",
    "        scores.append(np.mean(sims))\n",
    "    return np.mean(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42fe6a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def qa_precision_score(qa_dataset, model):\n",
    "    correct = 0\n",
    "    total = len(qa_dataset)\n",
    "    for qa in qa_dataset:\n",
    "        question = qa['question']\n",
    "        answer = qa['answer']\n",
    "        chunk_text = answer + \" \" + question  # Approximate original chunk\n",
    "\n",
    "        q_emb = model.encode([question])[0]\n",
    "        c_emb = model.encode([chunk_text])[0]\n",
    "        sim = cosine_similarity([q_emb], [c_emb])[0][0]\n",
    "        if sim > 0.65:\n",
    "            correct += 1\n",
    "    return correct / total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09503cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def redundancy_score(questions, model):\n",
    "    embeddings = model.encode(questions)\n",
    "    redundancy = 0\n",
    "    n = len(embeddings)\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            sim = cosine_similarity([embeddings[i]], [embeddings[j]])[0][0]\n",
    "            if sim > 0.9:\n",
    "                redundancy += 1\n",
    "    return redundancy / n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "516aeb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_nouns(text):\n",
    "    doc = nlp(text)\n",
    "    return {token.lemma_ for token in doc if token.pos_ in [\"NOUN\", \"PROPN\"]}\n",
    "\n",
    "def coverage_score(qa_dataset, full_text):\n",
    "    text_nouns = extract_nouns(full_text)\n",
    "    qa_nouns = set()\n",
    "    for qa in qa_dataset:\n",
    "        qa_nouns.update(extract_nouns(qa['question'] + \" \" + qa['answer']))\n",
    "    if not text_nouns:\n",
    "        return 0\n",
    "    return len(qa_nouns & text_nouns) / len(text_nouns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94a9bd6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Evaluation Metrics:\n",
      "🧠 Chunk Coherence : 1.000\n",
      "✅ QA Precision    : 0.988\n",
      "🔁 Redundancy      : 0.814\n",
      "📚 Coverage        : 0.329\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have raw document text available\n",
    "with open(\"datastore/phi-basic.pdf\", \"rb\") as f:\n",
    "    from pdfplumber import open as pdfopen\n",
    "    with pdfopen(f) as pdf:\n",
    "        full_text = \"\\n\".join(page.extract_text() for page in pdf.pages)\n",
    "\n",
    "coherence = chunk_coherence_score(answers, embedder)\n",
    "precision = qa_precision_score(qa_dataset, embedder)\n",
    "redundancy = redundancy_score(questions, embedder)\n",
    "coverage = coverage_score(qa_dataset, full_text)\n",
    "\n",
    "print(\"\\n📊 Evaluation Metrics:\")\n",
    "print(f\"🧠 Chunk Coherence : {coherence:.3f}\")\n",
    "print(f\"✅ QA Precision    : {precision:.3f}\")\n",
    "print(f\"🔁 Redundancy      : {redundancy:.3f}\")\n",
    "print(f\"📚 Coverage        : {coverage:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40c4768e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"raft_qa_dataset_agentic.json\") as f:\n",
    "    qa_dataset = json.load(f)\n",
    "\n",
    "chunks = list({(qa['source_file'], qa['plan_type']): [] for qa in qa_dataset})  # Placeholder for grouping by doc\n",
    "questions = [qa['question'] for qa in qa_dataset]\n",
    "answers = [qa['answer'] for qa in qa_dataset]\n",
    "all_qa_texts = [qa['question'] + \" \" + qa['answer'] for qa in qa_dataset]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5cb3663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Evaluation Metrics:\n",
      "🧠 Chunk Coherence : 0.956\n",
      "✅ QA Precision    : 0.908\n",
      "🔁 Redundancy      : 0.508\n",
      "📚 Coverage        : 0.285\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have raw document text available\n",
    "with open(\"datastore/phi-basic.pdf\", \"rb\") as f:\n",
    "    from pdfplumber import open as pdfopen\n",
    "    with pdfopen(f) as pdf:\n",
    "        full_text = \"\\n\".join(page.extract_text() for page in pdf.pages)\n",
    "\n",
    "coherence = chunk_coherence_score(answers, embedder)\n",
    "precision = qa_precision_score(qa_dataset, embedder)\n",
    "redundancy = redundancy_score(questions, embedder)\n",
    "coverage = coverage_score(qa_dataset, full_text)\n",
    "\n",
    "print(\"\\n📊 Evaluation Metrics:\")\n",
    "print(f\"🧠 Chunk Coherence : {coherence:.3f}\")\n",
    "print(f\"✅ QA Precision    : {precision:.3f}\")\n",
    "print(f\"🔁 Redundancy      : {redundancy:.3f}\")\n",
    "print(f\"📚 Coverage        : {coverage:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d5eb01a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Evaluation Metrics:\n",
      "🧠 Chunk Coherence : 0.921\n",
      "✅ QA Precision    : 0.902\n",
      "🔁 Redundancy      : 0.344\n",
      "📚 Coverage        : 0.257\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"raft_qa_dataset_llm_chunked.json\") as f:\n",
    "    qa_dataset = json.load(f)\n",
    "\n",
    "chunks = list({(qa['source_file'], qa['plan_type']): [] for qa in qa_dataset})  # Placeholder for grouping by doc\n",
    "questions = [qa['question'] for qa in qa_dataset]\n",
    "answers = [qa['answer'] for qa in qa_dataset]\n",
    "all_qa_texts = [qa['question'] + \" \" + qa['answer'] for qa in qa_dataset]\n",
    "\n",
    "# Assuming you have raw document text available\n",
    "with open(\"datastore/phi-basic.pdf\", \"rb\") as f:\n",
    "    from pdfplumber import open as pdfopen\n",
    "    with pdfopen(f) as pdf:\n",
    "        full_text = \"\\n\".join(page.extract_text() for page in pdf.pages)\n",
    "\n",
    "coherence = chunk_coherence_score(answers, embedder)\n",
    "precision = qa_precision_score(qa_dataset, embedder)\n",
    "redundancy = redundancy_score(questions, embedder)\n",
    "coverage = coverage_score(qa_dataset, full_text)\n",
    "\n",
    "print(\"\\n📊 Evaluation Metrics:\")\n",
    "print(f\"🧠 Chunk Coherence : {coherence:.3f}\")\n",
    "print(f\"✅ QA Precision    : {precision:.3f}\")\n",
    "print(f\"🔁 Redundancy      : {redundancy:.3f}\")\n",
    "print(f\"📚 Coverage        : {coverage:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21599a8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Evaluation Metrics:\n",
      "🧠 Chunk Coherence : 1.000\n",
      "✅ QA Precision    : 0.727\n",
      "🔁 Redundancy      : 0.818\n",
      "📚 Coverage        : 0.076\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"raft_qa_dataset_late_chunking.json\") as f:\n",
    "    qa_dataset = json.load(f)\n",
    "\n",
    "chunks = list({(qa['source_file'], qa['plan_type']): [] for qa in qa_dataset})  # Placeholder for grouping by doc\n",
    "questions = [qa['question'] for qa in qa_dataset]\n",
    "answers = [qa['answer'] for qa in qa_dataset]\n",
    "all_qa_texts = [qa['question'] + \" \" + qa['answer'] for qa in qa_dataset]\n",
    "\n",
    "# Assuming you have raw document text available\n",
    "with open(\"datastore/phi-basic.pdf\", \"rb\") as f:\n",
    "    from pdfplumber import open as pdfopen\n",
    "    with pdfopen(f) as pdf:\n",
    "        full_text = \"\\n\".join(page.extract_text() for page in pdf.pages)\n",
    "\n",
    "coherence = chunk_coherence_score(answers, embedder)\n",
    "precision = qa_precision_score(qa_dataset, embedder)\n",
    "redundancy = redundancy_score(questions, embedder)\n",
    "coverage = coverage_score(qa_dataset, full_text)\n",
    "\n",
    "print(\"\\n📊 Evaluation Metrics:\")\n",
    "print(f\"🧠 Chunk Coherence : {coherence:.3f}\")\n",
    "print(f\"✅ QA Precision    : {precision:.3f}\")\n",
    "print(f\"🔁 Redundancy      : {redundancy:.3f}\")\n",
    "print(f\"📚 Coverage        : {coverage:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "113667d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Evaluation Metrics:\n",
      "🧠 Chunk Coherence : 0.873\n",
      "✅ QA Precision    : 0.868\n",
      "🔁 Redundancy      : 0.824\n",
      "📚 Coverage        : 0.389\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"raft_qa_dataset_context_chunking.json\") as f:\n",
    "    qa_dataset = json.load(f)\n",
    "\n",
    "chunks = list({(qa['source_file'], qa['plan_type']): [] for qa in qa_dataset})  # Placeholder for grouping by doc\n",
    "questions = [qa['question'] for qa in qa_dataset]\n",
    "answers = [qa['answer'] for qa in qa_dataset]\n",
    "all_qa_texts = [qa['question'] + \" \" + qa['answer'] for qa in qa_dataset]\n",
    "\n",
    "# Assuming you have raw document text available\n",
    "with open(\"datastore/phi-basic.pdf\", \"rb\") as f:\n",
    "    from pdfplumber import open as pdfopen\n",
    "    with pdfopen(f) as pdf:\n",
    "        full_text = \"\\n\".join(page.extract_text() for page in pdf.pages)\n",
    "\n",
    "coherence = chunk_coherence_score(answers, embedder)\n",
    "precision = qa_precision_score(qa_dataset, embedder)\n",
    "redundancy = redundancy_score(questions, embedder)\n",
    "coverage = coverage_score(qa_dataset, full_text)\n",
    "\n",
    "print(\"\\n📊 Evaluation Metrics:\")\n",
    "print(f\"🧠 Chunk Coherence : {coherence:.3f}\")\n",
    "print(f\"✅ QA Precision    : {precision:.3f}\")\n",
    "print(f\"🔁 Redundancy      : {redundancy:.3f}\")\n",
    "print(f\"📚 Coverage        : {coverage:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "96a2eec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Evaluation Metrics:\n",
      "🧠 Chunk Coherence : 1.000\n",
      "✅ QA Precision    : 0.968\n",
      "🔁 Redundancy      : 6.690\n",
      "📚 Coverage        : 0.759\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"raft_qa_dataset_hybrid.json\") as f:\n",
    "    qa_dataset = json.load(f)\n",
    "\n",
    "chunks = list({(qa['source_file'], qa['plan_type']): [] for qa in qa_dataset})  # Placeholder for grouping by doc\n",
    "questions = [qa['question'] for qa in qa_dataset]\n",
    "answers = [qa['answer'] for qa in qa_dataset]\n",
    "all_qa_texts = [qa['question'] + \" \" + qa['answer'] for qa in qa_dataset]\n",
    "\n",
    "# Assuming you have raw document text available\n",
    "with open(\"datastore/phi-basic.pdf\", \"rb\") as f:\n",
    "    from pdfplumber import open as pdfopen\n",
    "    with pdfopen(f) as pdf:\n",
    "        full_text = \"\\n\".join(page.extract_text() for page in pdf.pages)\n",
    "\n",
    "coherence = chunk_coherence_score(answers, embedder)\n",
    "precision = qa_precision_score(qa_dataset, embedder)\n",
    "redundancy = redundancy_score(questions, embedder)\n",
    "coverage = coverage_score(qa_dataset, full_text)\n",
    "\n",
    "print(\"\\n📊 Evaluation Metrics:\")\n",
    "print(f\"🧠 Chunk Coherence : {coherence:.3f}\")\n",
    "print(f\"✅ QA Precision    : {precision:.3f}\")\n",
    "print(f\"🔁 Redundancy      : {redundancy:.3f}\")\n",
    "print(f\"📚 Coverage        : {coverage:.3f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
