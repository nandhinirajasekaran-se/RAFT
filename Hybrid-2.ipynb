{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "729ea303",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import tiktoken\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "tokenizer = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "def spacy_sent_tokenize(text):\n",
    "    doc = nlp(text)\n",
    "    return [sent.text.strip() for sent in doc.sents]\n",
    "\n",
    "\n",
    "def hybrid_semantic_context_chunk(text, token_limit=500, similarity_threshold=0.75):\n",
    "    sentences = spacy_sent_tokenize(text)\n",
    "    if not sentences:\n",
    "        return []\n",
    "\n",
    "    embeddings = embedder.encode(sentences)\n",
    "    chunks = []\n",
    "    current = [sentences[0]]\n",
    "    current_tokens = len(tokenizer.encode(sentences[0]))\n",
    "\n",
    "    for i in range(1, len(sentences)):\n",
    "        sim = cosine_similarity([embeddings[i-1]], [embeddings[i]])[0][0]\n",
    "        sentence_tokens = len(tokenizer.encode(sentences[i]))\n",
    "\n",
    "        # If adding this sentence will exceed token limit or it's not semantically close\n",
    "        if sim < similarity_threshold or (current_tokens + sentence_tokens > token_limit):\n",
    "            chunks.append(\" \".join(current))\n",
    "            current = [sentences[i]]\n",
    "            current_tokens = sentence_tokens\n",
    "        else:\n",
    "            current.append(sentences[i])\n",
    "            current_tokens += sentence_tokens\n",
    "\n",
    "    if current:\n",
    "        chunks.append(\" \".join(current))\n",
    "\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dcb65c8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Processing: phi-basic.pdf\n",
      "\n",
      "‚úÖ Saved 792 QA pairs to raft_qa_dataset_hybrid.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import tiktoken\n",
    "\n",
    "# === CONFIG ===\n",
    "DOCS_DIR = \"datastore/\"\n",
    "OUTPUT_FILE = \"raft_qa_dataset_hybrid.json\"\n",
    "MODEL_NAME = \"gpt-3.5-turbo\"\n",
    "TOKEN_LIMIT = 600\n",
    "OVERLAP = 30\n",
    "SIM_THRESHOLD = 0.75\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "tokenizer = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "def spacy_sent_tokenize(text):\n",
    "    doc = nlp(text)\n",
    "    return [sent.text.strip() for sent in doc.sents]\n",
    "\n",
    "# === COMPONENTS ===\n",
    "llm = ChatOpenAI(model_name=MODEL_NAME, temperature=0.5)\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "tokenizer = tiktoken.encoding_for_model(MODEL_NAME)\n",
    "\n",
    "qa_prompt = PromptTemplate(\n",
    "    input_variables=[\"chunk\", \"metadata\"],\n",
    "    template=\"\"\"\n",
    "You are a JSON generator for question-answer pairs. Given the document chunk and metadata below, generate 2-3 QA pairs in valid JSON format. Output ONLY the JSON array, with no additional text, explanations, or markdown.\n",
    "\n",
    "Chunk: {chunk}\n",
    "Metadata: {metadata}\n",
    "\n",
    "Example output:\n",
    "[{{\"question\": \"What is the drug reimbursement rate?\", \"answer\": \"60% reimbursement\"}},\n",
    " {{\"question\": \"What is the annual maximum?\", \"answer\": \"$750\"}}]\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# === HELPERS ===\n",
    "def is_complex_chunk(text):\n",
    "    return bool(re.search(r\"exclusion|limitation|condition\", text, re.IGNORECASE))\n",
    "\n",
    "def clean_json_response(response):\n",
    "    if hasattr(response, \"content\"):\n",
    "        response = response.content\n",
    "    try:\n",
    "        return json.loads(response)\n",
    "    except json.JSONDecodeError:\n",
    "        start = response.find(\"[\")\n",
    "        end = response.rfind(\"]\") + 1\n",
    "        if start != -1 and end != -1:\n",
    "            try:\n",
    "                return json.loads(response[start:end])\n",
    "            except:\n",
    "                pass\n",
    "        print(\"‚ö†Ô∏è Invalid response:\\n\", response)\n",
    "        return []\n",
    "\n",
    "def generate_llm_qa(text, metadata):\n",
    "    try:\n",
    "        prompt = qa_prompt.format_prompt(\n",
    "            chunk=text[:1000],\n",
    "            metadata=json.dumps(metadata)\n",
    "        ).to_string()\n",
    "        response = llm([HumanMessage(content=prompt)])\n",
    "        return clean_json_response(response)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå LLM QA generation failed: {e}\")\n",
    "        return []\n",
    "\n",
    "def hybrid_semantic_context_chunk(text, token_limit=TOKEN_LIMIT, similarity_threshold=SIM_THRESHOLD):\n",
    "    sentences = spacy_sent_tokenize(text)\n",
    "    if not sentences:\n",
    "        return []\n",
    "\n",
    "    embeddings = embedder.encode(sentences)\n",
    "    chunks = []\n",
    "    current = [sentences[0]]\n",
    "    current_tokens = len(tokenizer.encode(sentences[0]))\n",
    "\n",
    "    for i in range(1, len(sentences)):\n",
    "        sim = cosine_similarity([embeddings[i-1]], [embeddings[i]])[0][0]\n",
    "        sentence_tokens = len(tokenizer.encode(sentences[i]))\n",
    "\n",
    "        if sim < similarity_threshold or (current_tokens + sentence_tokens > token_limit):\n",
    "            chunks.append(\" \".join(current))\n",
    "            current = [sentences[i]]\n",
    "            current_tokens = sentence_tokens\n",
    "        else:\n",
    "            current.append(sentences[i])\n",
    "            current_tokens += sentence_tokens\n",
    "\n",
    "    if current:\n",
    "        chunks.append(\" \".join(current))\n",
    "\n",
    "    return chunks\n",
    "\n",
    "# === MAIN PIPELINE ===\n",
    "qa_dataset = []\n",
    "\n",
    "for pdf_file in os.listdir(DOCS_DIR):\n",
    "    if not pdf_file.endswith(\".pdf\"):\n",
    "        continue\n",
    "\n",
    "    print(f\"üìÑ Processing: {pdf_file}\")\n",
    "    file_path = os.path.join(DOCS_DIR, pdf_file)\n",
    "    loader = PyPDFLoader(file_path)\n",
    "    pages = loader.load()\n",
    "\n",
    "    plan_type = \"basic\" if \"basic\" in pdf_file.lower() else \"standard\" if \"standard\" in pdf_file.lower() else \"enhanced\"\n",
    "\n",
    "    for page in pages:\n",
    "        chunks = hybrid_semantic_context_chunk(page.page_content)\n",
    "        for chunk in chunks:\n",
    "            metadata = {\"source_file\": pdf_file, \"plan_type\": plan_type}\n",
    "            qa_pairs = generate_llm_qa(chunk, metadata)\n",
    "\n",
    "            for qa in qa_pairs:\n",
    "                if not isinstance(qa, dict) or \"question\" not in qa or \"answer\" not in qa:\n",
    "                    print(\"‚ö†Ô∏è Skipping malformed QA:\", qa)\n",
    "                    continue\n",
    "                qa_dataset.append({\n",
    "                    \"question\": qa[\"question\"],\n",
    "                    \"answer\": qa[\"answer\"],\n",
    "                    \"source_file\": metadata[\"source_file\"],\n",
    "                    \"plan_type\": metadata[\"plan_type\"]\n",
    "                })\n",
    "\n",
    "# === SAVE OUTPUT ===\n",
    "with open(OUTPUT_FILE, \"w\") as f:\n",
    "    json.dump(qa_dataset, f, indent=2)\n",
    "\n",
    "print(f\"\\n‚úÖ Saved {len(qa_dataset)} QA pairs to {OUTPUT_FILE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2386a1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def deduplicate_qa_by_question(qa_dataset, threshold=0.8):\n",
    "    embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "    questions = [qa[\"question\"] for qa in qa_dataset]\n",
    "    \n",
    "    # ‚¨áÔ∏è Move tensor from MPS to CPU before numpy conversion\n",
    "    embeddings = embedder.encode(questions, convert_to_tensor=True).cpu().numpy()\n",
    "    sim_matrix = cosine_similarity(embeddings)\n",
    "\n",
    "    keep_indices = []\n",
    "    seen = set()\n",
    "\n",
    "    for i in range(len(qa_dataset)):\n",
    "        if i in seen:\n",
    "            continue\n",
    "        keep_indices.append(i)\n",
    "        for j in range(i + 1, len(qa_dataset)):\n",
    "            if sim_matrix[i][j] >= threshold:\n",
    "                seen.add(j)\n",
    "\n",
    "    return [qa_dataset[i] for i in keep_indices]\n",
    "\n",
    "\n",
    "def exact_deduplicate(qa_dataset):\n",
    "    seen = set()\n",
    "    deduped = []\n",
    "    for qa in qa_dataset:\n",
    "        key = (qa[\"question\"].strip().lower(), qa[\"answer\"].strip().lower())\n",
    "        if key not in seen:\n",
    "            seen.add(key)\n",
    "            deduped.append(qa)\n",
    "    return deduped\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cf38aabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_dataset_new = exact_deduplicate(qa_dataset)\n",
    "qa_dataset_new1 = deduplicate_qa_by_question(qa_dataset_new, threshold=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "71e95b3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Saved 792 QA pairs to raft_qa_dataset_hybrid_v1.json\n"
     ]
    }
   ],
   "source": [
    "# === SAVE OUTPUT ===\n",
    "\n",
    "OUTPUT_FILE = \"raft_qa_dataset_hybrid_v1.json\"\n",
    "\n",
    "with open(OUTPUT_FILE, \"w\") as f:\n",
    "    json.dump(qa_dataset_new1, f, indent=2)\n",
    "\n",
    "print(f\"\\n‚úÖ Saved {len(qa_dataset)} QA pairs to {OUTPUT_FILE}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
