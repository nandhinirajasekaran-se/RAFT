{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "64e590e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved:\n",
      " ['4 \\nDrug provision \\nPrescription drugs \\nDrugs covered under this plan must have a Drug Identification Number (DIN). \\nWe will cover the cost of the following drugs and supplies that are prescribed by a physician or \\ndentist and are obtained from a pharmacist:  \\n‚Ä¢ drugs that legally require a prescript', '2 \\nPlan summary  \\n \\n \\nNote: \\nWe will only reimburse medical expenses that are not covered by the insured person‚Äôs provincial \\no\\nr territorial health care plan. \\n \\n \\nDrug \\nThe amount we pay for the dispensing fee reimbursement is 100% but is limited to a maximum of $5 \\np\\ner prescription. \\nDrug  (for ', '10 \\nWe confirm whether the ex pense you submitted is an eligible expense. We determine if there are \\nany limitations and exclusions which are described in the applicable provisions. If any of the \\nexpenses aren‚Äôt eligible, we subtract that expense from the total amount you are claiming. \\nFor each el']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/k9/ywt32ng54d766jxqldqb4y7c0000gn/T/ipykernel_37435/103011523.py:65: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = llm([HumanMessage(content=formatted_prompt)]).content\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üßæ Raw LLM Output:\n",
      " [\n",
      "    {\"question\": \"What types of drugs will not be paid for even when prescribed?\", \"answer\": \"Drugs for the treatment of infertility, drugs for the treatment of sexual dysfunction, anti-obesity drugs, dietary supplements, infant formulas, minerals, proteins, vitamins, collagen treatments, contraceptives, the cost of giving injections, serums, vaccines, over-the-counter products designed to help quit smoking\"},\n",
      "    {\"question\": \"Where can I find more information about what expenses will not be covered?\", \"answer\": \"In the 'When we will not pay (exclusions)' section of the Other information about your policy pages\"},\n",
      "    {\"question\": \"What are some examples of expenses that will not be covered?\", \"answer\": \"Expenses incurred under any of the conditions specified in the 'When we will not pay (exclusions)' section of the Other information about your policy pages\"}\n",
      "]\n",
      "\n",
      "üßæ Raw LLM Output:\n",
      " [\n",
      "    {\"question\": \"What does the document determine?\", \"answer\": \"Limitations and exclusions described in applicable provisions\"},\n",
      "    {\"question\": \"What is the source file of the document?\", \"answer\": \"phi-basic.pdf\"},\n",
      "    {\"question\": \"What type of plan is described in the document?\", \"answer\": \"Basic\"}\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.schema import Document\n",
    "import spacy\n",
    "import re\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import nltk\n",
    "import pdfplumber\n",
    "from dotenv import load_dotenv\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "qa_prompt = PromptTemplate(\n",
    "    input_variables=[\"chunk\", \"metadata\"],\n",
    "    template=\"\"\"\n",
    "    You are a JSON generator for question-answer pairs. Given the document chunk and metadata below, generate 2-3 QA pairs in valid JSON format. Output ONLY the JSON array.\n",
    "\n",
    "    Chunk: {chunk}\n",
    "    Metadata: {metadata}\n",
    "\n",
    "    Example output:\n",
    "    [{{\"question\": \"What is the drug reimbursement rate?\", \"answer\": \"60% reimbursement\"}},\n",
    "     {{\"question\": \"What is the annual maximum?\", \"answer\": \"$750\"}}]\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.5)\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "loader = PyPDFLoader(\"datastore/phi-basic.pdf\")\n",
    "pages = loader.load()  # Each page is one large block\n",
    "\n",
    "vector_store = FAISS.from_documents(pages, embedding_model)\n",
    "\n",
    "\n",
    "\n",
    "retrieved_docs = vector_store.similarity_search(\"drug coverage\", k=3)\n",
    "print(\"Retrieved:\\n\", [d.page_content[:300] for d in retrieved_docs])\n",
    "\n",
    "#from nltk.tokenize import sent_tokenize\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "def is_complex_chunk(text):\n",
    "    return bool(re.search(r'exclusion|limitation|condition', text, re.IGNORECASE))\n",
    "\n",
    "def generate_llm_qa(text, metadata):\n",
    "    try:\n",
    "        formatted_prompt = qa_prompt.format_prompt(\n",
    "            chunk=text[:1000],\n",
    "            metadata=json.dumps(metadata)\n",
    "        ).to_string()\n",
    "\n",
    "        response = llm([HumanMessage(content=formatted_prompt)]).content\n",
    "        print(\"\\nüßæ Raw LLM Output:\\n\", response)\n",
    "\n",
    "        try:\n",
    "            qa_list = json.loads(response)\n",
    "        except:\n",
    "            start, end = response.find(\"[\"), response.rfind(\"]\") + 1\n",
    "            qa_list = json.loads(response[start:end]) if start != -1 and end != -1 else []\n",
    "\n",
    "        valid_qas = []\n",
    "        for qa in qa_list:\n",
    "            if isinstance(qa, dict) and \"question\" in qa and \"answer\" in qa:\n",
    "                valid_qas.append(qa)\n",
    "            else:\n",
    "                print(\"‚ö†Ô∏è Skipping malformed QA:\", repr(qa))\n",
    "        return valid_qas\n",
    "    except Exception as e:\n",
    "        print(\"‚ùå LLM QA generation failed:\", e)\n",
    "        return []\n",
    "\n",
    "def generate_template_qa(text, metadata):\n",
    "    pattern = r'(\\w+)\\s+reimbursement|maximum of \\$([\\d,]+)'\n",
    "    matches = re.findall(pattern, text)\n",
    "    qa_pairs = []\n",
    "    for m in matches:\n",
    "        entity = m[0] or m[1]\n",
    "        question = f\"What is the {entity} reimbursement rate for the {metadata['plan_type']} plan?\"\n",
    "        qa_pairs.append({\"question\": question, \"answer\": text[:200]})\n",
    "    return qa_pairs\n",
    "\n",
    "\n",
    "def spacy_sent_tokenize(text):\n",
    "    doc = nlp(text)\n",
    "    return [sent.text.strip() for sent in doc.sents]\n",
    "\n",
    "def semantic_chunk(text, model, threshold=0.75):\n",
    "    sentences = spacy_sent_tokenize(text)\n",
    "    if not sentences:\n",
    "        return []\n",
    "    embeddings = model.encode(sentences)\n",
    "    chunks = []\n",
    "    current = [sentences[0]]\n",
    "    for i in range(1, len(sentences)):\n",
    "        sim = cosine_similarity([embeddings[i-1]], [embeddings[i]])[0][0]\n",
    "        if sim < threshold:\n",
    "            chunks.append(\" \".join(current))\n",
    "            current = [sentences[i]]\n",
    "        else:\n",
    "            current.append(sentences[i])\n",
    "    if current:\n",
    "        chunks.append(\" \".join(current))\n",
    "    return chunks\n",
    "\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "late_chunks = []\n",
    "for doc in retrieved_docs:\n",
    "    late_chunks.extend(semantic_chunk(doc.page_content, embedder))\n",
    "\n",
    "\n",
    "#from nltk.tokenize import sent_tokenize\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "def spacy_sent_tokenize(text):\n",
    "    doc = nlp(text)\n",
    "    return [sent.text.strip() for sent in doc.sents]\n",
    "\n",
    "def semantic_chunk(text, model, threshold=0.75):\n",
    "    sentences = spacy_sent_tokenize(text)\n",
    "    if not sentences:\n",
    "        return []\n",
    "    embeddings = model.encode(sentences)\n",
    "    chunks = []\n",
    "    current = [sentences[0]]\n",
    "    for i in range(1, len(sentences)):\n",
    "        sim = cosine_similarity([embeddings[i-1]], [embeddings[i]])[0][0]\n",
    "        if sim < threshold:\n",
    "            chunks.append(\" \".join(current))\n",
    "            current = [sentences[i]]\n",
    "        else:\n",
    "            current.append(sentences[i])\n",
    "    if current:\n",
    "        chunks.append(\" \".join(current))\n",
    "    return chunks\n",
    "\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "late_chunks = []\n",
    "for doc in retrieved_docs:\n",
    "    late_chunks.extend(semantic_chunk(doc.page_content, embedder))\n",
    "\n",
    "qa_dataset = []\n",
    "for chunk in late_chunks:\n",
    "    metadata = {\"source_file\": \"phi-basic.pdf\", \"plan_type\": \"basic\"}\n",
    "    qa_pairs = generate_llm_qa(chunk, metadata) if is_complex_chunk(chunk) else generate_template_qa(chunk, metadata)\n",
    "\n",
    "    for qa in qa_pairs:\n",
    "        if isinstance(qa, dict) and \"question\" in qa and \"answer\" in qa:\n",
    "            qa_dataset.append({\n",
    "                \"question\": qa[\"question\"],\n",
    "                \"answer\": qa[\"answer\"],\n",
    "                \"source_file\": \"phi-basic.pdf\",\n",
    "                \"plan_type\": \"basic\"\n",
    "            })\n",
    "\n",
    "with open(\"raft_qa_dataset_late_chunking.json\", \"w\") as f:\n",
    "    json.dump(qa_dataset, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49b34346",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.11.0.post1-cp312-cp312-macosx_14_0_arm64.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in ./.venv/lib/python3.12/site-packages (from faiss-cpu) (2.3.2)\n",
      "Requirement already satisfied: packaging in ./.venv/lib/python3.12/site-packages (from faiss-cpu) (25.0)\n",
      "Downloading faiss_cpu-1.11.0.post1-cp312-cp312-macosx_14_0_arm64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: faiss-cpu\n",
      "Successfully installed faiss-cpu-1.11.0.post1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755e3fd7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
